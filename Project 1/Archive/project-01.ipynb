{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "Amit Talapatra\n",
    "\n",
    "9/16/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 - Word counts (40 points)\n",
    "#### Part A. Characters in Little Women\n",
    "First, I downloaded the text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-21 15:12:15--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/women.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.20.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.20.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1053440 (1.0M) [text/plain]\n",
      "Saving to: ‘women.txt’\n",
      "\n",
      "women.txt           100%[=====================>]   1.00M  --.-KB/s   in 0.08s  \n",
      "\n",
      "2016-09-21 15:12:15 (12.9 MB/s) - ‘women.txt’ saved [1053440/1053440]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/women.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How many times are each of the following characters mentioned by name in the text of Little Women? Jo, Beth, Meg, Amy\n",
    "\n",
    "I converted the words in the document to lowercase and searched the text for each of the four names, counting each instance of these names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    652 amy\r\n",
      "    467 beth\r\n",
      "   1362 jo\r\n",
      "    686 meg\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt | grep -oE '\\w{{2,}}' | tr '[:upper:]' '[:lower:]' | grep -o -w 'jo\\|beth\\|meg\\|amy' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B. Juliet and Romeo in Romeo and Juliet\n",
    "First, I downloaded the text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-21 15:13:42--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/romeo.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.20.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.20.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 178983 (175K) [text/plain]\n",
      "Saving to: ‘romeo.txt’\n",
      "\n",
      "romeo.txt           100%[=====================>] 174.79K  --.-KB/s   in 0.02s  \n",
      "\n",
      "2016-09-21 15:13:42 (7.53 MB/s) - ‘romeo.txt’ saved [178983/178983]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/romeo.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How many times do each of the characters Juliet and Romeo have speaking lines in Romeo and Juliet?\n",
    "\n",
    "Reading the text, speaking lines for each character are marked by \"Rom\" and \"Jul\", so I used these to pull the relevant lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    117 Jul\r\n",
      "    163 Rom\r\n"
     ]
    }
   ],
   "source": [
    "!cat romeo.txt | grep -oE '\\w{{2,}}' |  grep -w 'Rom\\|Jul' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 - Capital Bikeshare (40 points)\n",
    "First, I downloaded and unzipped the csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-21 15:16:28--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/2016q1.csv.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.20.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.20.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10643003 (10M) [application/octet-stream]\n",
      "Saving to: ‘2016q1.csv.zip’\n",
      "\n",
      "2016q1.csv.zip      100%[=====================>]  10.15M  48.6MB/s   in 0.2s   \n",
      "\n",
      "2016-09-21 15:16:28 (48.6 MB/s) - ‘2016q1.csv.zip’ saved [10643003/10643003]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/2016q1.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  2016q1.csv.zip\n",
      "  inflating: 2016q1.csv              \n"
     ]
    }
   ],
   "source": [
    "!unzip 2016q1.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A (20 points)\n",
    "##### Which 10 Capital Bikeshare stations were the most popular departing stations in Q1 2016?\n",
    "Using 'csvcut', I isolated the column of departing stations and sorted by count to determine the most popular one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  13120 Columbus Circle / Union Station\r\n",
      "   9560 Massachusetts Ave & Dupont Circle NW\r\n",
      "   9388 Lincoln Memorial\r\n",
      "   8138 Jefferson Dr & 14th St SW\r\n",
      "   7479 Thomas Circle\r\n",
      "   7401 15th & P St NW\r\n",
      "   6568 14th & V St NW\r\n",
      "   6491 New Hampshire Ave & T St NW\r\n",
      "   5649 Eastern Market Metro / Pennsylvania Ave & 7th St SE\r\n",
      "   5514 17th & Corcoran St NW\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c5 2016q1.csv | sort | uniq -c | sort -rn | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which 10 were the most popular destination stations in Q1 2016?\n",
    "I used the same method as above, this time using the column of destination stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  13880 Columbus Circle / Union Station\r\n",
      "  11183 Massachusetts Ave & Dupont Circle NW\r\n",
      "   9419 Lincoln Memorial\r\n",
      "   8975 Jefferson Dr & 14th St SW\r\n",
      "   8092 15th & P St NW\r\n",
      "   7267 14th & V St NW\r\n",
      "   6997 Thomas Circle\r\n",
      "   6245 New Hampshire Ave & T St NW\r\n",
      "   5761 5th & K St NW\r\n",
      "   5651 17th & Corcoran St NW\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c7 2016q1.csv | sort | uniq -c | sort -rn | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B (20 points)\n",
    "##### For the most popular departure station, which 10 bikes were used most in trips departing from there? \n",
    "I identified \"Columbus Circle / Union Station\" as the most popular departure and destination station. Using csvcut to isolate the departure/destination columns and the column of bike numbers, I cut out the column of bike numbers, sorted it, and counted to identify the most used bikes from the station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17 W22227\r\n",
      "     16 W21867\r\n",
      "     16 W21641\r\n",
      "     16 W21538\r\n",
      "     16 W21239\r\n",
      "     16 W20540\r\n",
      "     16 W00714\r\n",
      "     15 W22080\r\n",
      "     15 W21450\r\n",
      "     15 W21076\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c5,8 2016q1.csv | csvgrep -c1 -m 'Columbus Circle / Union Station' | csvcut -c2 | csvsort -c1 | uniq -c | sort -rn | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which 10 bikes were used most in trips ending at the most popular destination station?\n",
    "I used the same methodology described above, but for the destination station column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18 W00485\r\n",
      "     17 W22227\r\n",
      "     16 W22099\r\n",
      "     16 W22080\r\n",
      "     16 W21239\r\n",
      "     16 W21076\r\n",
      "     16 W20425\r\n",
      "     16 W00714\r\n",
      "     15 W21997\r\n",
      "     15 W21867\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c7,8 2016q1.csv | csvgrep -c1 -m 'Columbus Circle / Union Station' | csvcut -c2 | csvsort -c1 | uniq -c | sort -rn | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 - Filters (20 points)\n",
    "#### Part A (10 points)\n",
    "##### Demonstrate a pipeline that performs a count of the top ten unique words in Little Women. This may be exactly the same pipeline we have used before.\n",
    "The pipeline is shown below. This uses 'grep' and 'tr', as we have used in previous examples. The instruction to split lines into words uses '\\w{{1,}}' so one letter words are not removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8155 and\r\n",
      "   7689 the\r\n",
      "   5152 to\r\n",
      "   4531 a\r\n",
      "   4003 i\r\n",
      "   3523 of\r\n",
      "   3245 her\r\n",
      "   2774 it\r\n",
      "   2503 in\r\n",
      "   2447 you\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt | grep -oE '\\w{{1,}}' | tr '[:upper:]' '[:lower:]' | sort | uniq -c | sort -rn | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write a Python filter than replaces grep -oE '\\w{2,}' to split lines of text into one word per line.\n",
    "This uses the splitlines.py python script. I used the 're' module to identify words because this mirrors the use of 'grep' in the UNIX command version of this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x splitlines.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8155 and\r\n",
      "   7689 the\r\n",
      "   5152 to\r\n",
      "   4531 a\r\n",
      "   4003 i\r\n",
      "   3523 of\r\n",
      "   3245 her\r\n",
      "   2774 it\r\n",
      "   2503 in\r\n",
      "   2447 you\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt | ./splitlines.py | tr '[:upper:]' '[:lower:]' | sort | uniq -c | sort -rn | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write an additional Python filter to replace tr '[:upper:]' '[:lower:]' to transform text into lower case.\n",
    "The lower.py script converts the lines to lowercase and strips spaces to mirror the use of 'tr' in the UNIX command version of this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x lower.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8155 and\r\n",
      "   7689 the\r\n",
      "   5152 to\r\n",
      "   4531 a\r\n",
      "   4003 i\r\n",
      "   3523 of\r\n",
      "   3245 her\r\n",
      "   2774 it\r\n",
      "   2503 in\r\n",
      "   2447 you\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt | ./splitlines.py | ./lower.py | sort | uniq -c | sort -rn | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B (10 points)\n",
    "##### Write a Python filter that removes at least ten common words of English text, commonly known as “stop words”. Sources of English stop word lists are readily available online, or you may generate your own list from the text.\n",
    "I created a list of more than ten stop words in stopwords.py and removed these if they appeared in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x stopwords.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add your stop word filter to a word count pipeline and show the top 25 words in Little Women with stop words removed.\n",
    "The results of using stopwords.py in the pipeline are shown below. Looking at this list, it appears that an unintended consequence of the method used is that contractions like \"don't\" or \"Jo's\" split the other side of the apostrophe as a separate word and added these to the list. These could have been added as exceptions to the python script, but it may require additional examination of the document to find all of the contractions that come up in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3245 her\r\n",
      "   2447 you\r\n",
      "   2343 she\r\n",
      "   1680 t\r\n",
      "   1598 he\r\n",
      "   1495 s\r\n",
      "   1469 but\r\n",
      "   1362 jo\r\n",
      "   1135 so\r\n",
      "   1118 his\r\n",
      "   1063 had\r\n",
      "    942 not\r\n",
      "    916 if\r\n",
      "    881 all\r\n",
      "    843 my\r\n",
      "    827 said\r\n",
      "    782 him\r\n",
      "    755 me\r\n",
      "    730 little\r\n",
      "    725 one\r\n",
      "    719 they\r\n",
      "    717 have\r\n",
      "    709 when\r\n",
      "    708 do\r\n",
      "    686 meg\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt | ./splitlines.py | ./lower.py | ./stopwords.py | sort | uniq -c | sort -rn | head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Extra credit (10 points)\n",
    "##### Use GNU parallel to count the 25 most common words across all the 109 texts in the zip file provided, with stop words removed. You may re-use your filters from Problem 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-21 23:07:18--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/texts.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.20.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.20.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12668137 (12M) [application/octet-stream]\n",
      "Saving to: ‘texts.zip’\n",
      "\n",
      "texts.zip           100%[=====================>]  12.08M  47.8MB/s   in 0.3s   \n",
      "\n",
      "2016-09-21 23:07:19 (47.8 MB/s) - ‘texts.zip’ saved [12668137/12668137]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/texts.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  texts.zip\n",
      "  inflating: many-texts/10001.txt    \n",
      "  inflating: many-texts/10002.txt    \n",
      "  inflating: many-texts/10003.txt    \n",
      "  inflating: many-texts/10004.txt    \n",
      "  inflating: many-texts/10005.txt    \n",
      "  inflating: many-texts/10006.txt    \n",
      "  inflating: many-texts/10007.txt    \n",
      "  inflating: many-texts/10008.txt    \n",
      "  inflating: many-texts/10009.txt    \n",
      "  inflating: many-texts/10010.txt    \n",
      "  inflating: many-texts/10011.txt    \n",
      "  inflating: many-texts/10012.txt    \n",
      "  inflating: many-texts/10013.txt    \n",
      "  inflating: many-texts/10014.txt    \n",
      "  inflating: many-texts/10015.txt    \n",
      "  inflating: many-texts/10016.txt    \n",
      "  inflating: many-texts/10017.txt    \n",
      "  inflating: many-texts/10018.txt    \n",
      "  inflating: many-texts/10019.txt    \n",
      "  inflating: many-texts/10020.txt    \n",
      "  inflating: many-texts/10021.txt    \n",
      "  inflating: many-texts/10023.txt    \n",
      "  inflating: many-texts/10024.txt    \n",
      "  inflating: many-texts/10025.txt    \n",
      "  inflating: many-texts/10026.txt    \n",
      "  inflating: many-texts/10027.txt    \n",
      "  inflating: many-texts/10028.txt    \n",
      "  inflating: many-texts/10029.txt    \n",
      "  inflating: many-texts/10030.txt    \n",
      "  inflating: many-texts/10031.txt    \n",
      "  inflating: many-texts/10032.txt    \n",
      "  inflating: many-texts/10033.txt    \n",
      "  inflating: many-texts/10034.txt    \n",
      "  inflating: many-texts/10035.txt    \n",
      "  inflating: many-texts/10036.txt    \n",
      "  inflating: many-texts/10037.txt    \n",
      "  inflating: many-texts/10038.txt    \n",
      "  inflating: many-texts/10039.txt    \n",
      "  inflating: many-texts/10040.txt    \n",
      "  inflating: many-texts/10041.txt    \n",
      "  inflating: many-texts/10042.txt    \n",
      "  inflating: many-texts/10043.txt    \n",
      "  inflating: many-texts/10045.txt    \n",
      "  inflating: many-texts/10046.txt    \n",
      "  inflating: many-texts/10047.txt    \n",
      "  inflating: many-texts/10048.txt    \n",
      "  inflating: many-texts/10049.txt    \n",
      "  inflating: many-texts/10050.txt    \n",
      "  inflating: many-texts/10051.txt    \n",
      "  inflating: many-texts/10052.txt    \n",
      "  inflating: many-texts/10056.txt    \n",
      "  inflating: many-texts/10059.txt    \n",
      "  inflating: many-texts/10060.txt    \n",
      "  inflating: many-texts/10062.txt    \n",
      "  inflating: many-texts/12370.txt    \n",
      "  inflating: many-texts/12372.txt    \n",
      "  inflating: many-texts/12373.txt    \n",
      "  inflating: many-texts/12374.txt    \n",
      "  inflating: many-texts/12375.txt    \n",
      "  inflating: many-texts/12376.txt    \n",
      "  inflating: many-texts/12377.txt    \n",
      "  inflating: many-texts/12378.txt    \n",
      "  inflating: many-texts/12380.txt    \n",
      "  inflating: many-texts/12381.txt    \n",
      "  inflating: many-texts/12383.txt    \n",
      "  inflating: many-texts/12384.txt    \n",
      "  inflating: many-texts/12385.txt    \n",
      "  inflating: many-texts/12386.txt    \n",
      "  inflating: many-texts/1jcfs10.txt  \n",
      "  inflating: many-texts/2babb10.txt  \n",
      "  inflating: many-texts/3babb10.txt  \n",
      "  inflating: many-texts/50bab10.txt  \n",
      "  inflating: many-texts/ajtl10.txt   \n",
      "  inflating: many-texts/allyr10.txt  \n",
      "  inflating: many-texts/alpsn10.txt  \n",
      "  inflating: many-texts/balen10.txt  \n",
      "  inflating: many-texts/baleng2.txt  \n",
      "  inflating: many-texts/batlf10.txt  \n",
      "  inflating: many-texts/bgopr10.txt  \n",
      "  inflating: many-texts/brnte10.txt  \n",
      "  inflating: many-texts/bstjg10.txt  \n",
      "  inflating: many-texts/cambp10.txt  \n",
      "  inflating: many-texts/canbe10.txt  \n",
      "  inflating: many-texts/cantp10.txt  \n",
      "  inflating: many-texts/cfrz10.txt   \n",
      "  inflating: many-texts/crsnk10.txt  \n",
      "  inflating: many-texts/esbio10.txt  \n",
      "  inflating: many-texts/grybr10.txt  \n",
      "  inflating: many-texts/mklmt10.txt  \n",
      "  inflating: many-texts/morem10.txt  \n",
      "  inflating: many-texts/mspcd10.txt  \n",
      "  inflating: many-texts/penbr10.txt  \n",
      "  inflating: many-texts/pgjr10.txt   \n",
      "  inflating: many-texts/pntvw10.txt  \n",
      "  inflating: many-texts/prcpg10.txt  \n",
      "  inflating: many-texts/prhg10.txt   \n",
      "  inflating: many-texts/prhsb10.txt  \n",
      "  inflating: many-texts/rlsl110.txt  \n",
      "  inflating: many-texts/rlsl210.txt  \n",
      "  inflating: many-texts/rmlav10.txt  \n",
      "  inflating: many-texts/sesli10.txt  \n",
      "  inflating: many-texts/svyrd10.txt  \n",
      "  inflating: many-texts/tecom10.txt  \n",
      "  inflating: many-texts/utrkj10.txt  \n",
      "  inflating: many-texts/vpasm10.txt  \n",
      "  inflating: many-texts/wldsp10.txt  \n",
      "  inflating: many-texts/wtrbs10.txt  \n",
      "  inflating: many-texts/zncli10.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip -d many-texts texts.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computers / CPU cores / Max jobs to run\n",
      "1:local / 2 / 2\n",
      "\n",
      "Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete\n",
      "ETA: 12s 58left 0.20avg  local:2/50/100%/0.2s Traceback (most recent call last):\n",
      "  File \"./splitlines.py\", line 15, in <module>\n",
      "    for line in fileinput.input():\n",
      "  File \"/opt/conda/lib/python3.5/fileinput.py\", line 248, in __next__\n",
      "    line = self._readline()\n",
      "  File \"/opt/conda/lib/python3.5/codecs.py\", line 321, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe9 in position 2423: invalid continuation byte\n",
      "ETA: 11s 56left 0.21avg  local:2/52/100%/0.2s "
     ]
    }
   ],
   "source": [
    "!ls many-texts/*.txt | parallel --eta -j+0 ./splitlines.py | ./lower.py | ./stopwords.py | sort | uniq -c | sort -rn | head -25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
